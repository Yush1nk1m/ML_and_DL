{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text\n",
    "                       if char not in string.punctuation)\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = { \"\": 0, \"[UNK]\": 1 }\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence\n",
    "        )\n",
    "\n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sequence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sequence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\"\n",
    "    )\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "inverse_vocabulary = dict(enumerate(vocabulary))\n",
    "decoded_sentence = \" \".join(inverse_vocabulary[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      " 17 80.2M   17 13.7M    0     0   143k      0  0:09:30  0:01:37  0:07:53     0:07:34  0:00:54  0:06:40  201k^C\n",
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: 아카이브에 예기치 않은 파일 끝 문자\n",
      "tar: 아카이브에 예기치 않은 파일 끝 문자\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "!rm -f aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:30:56.558083: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-16 17:30:56.593600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 17:30:56.593635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 17:30:56.594447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-16 17:30:56.600664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 17:30:57.452594: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 17:30:58.778162: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.807946: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.808117: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.808859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.808990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.809072: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.874713: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.874886: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.874978: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-16 17:30:58.875049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:03:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b\"I would love to have that two hours of my life back. It seemed to be several clips from Steve's Animal Planet series that was spliced into a loosely constructed script. Don't Go, If you must see it, wait for the video ...\", shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens, ))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 5ms/step - loss: 0.4151 - accuracy: 0.8242 - val_loss: 0.2836 - val_accuracy: 0.8872\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2745 - accuracy: 0.8980 - val_loss: 0.2692 - val_accuracy: 0.8924\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2386 - accuracy: 0.9170 - val_loss: 0.2790 - val_accuracy: 0.8984\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2247 - accuracy: 0.9231 - val_loss: 0.2968 - val_accuracy: 0.8940\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2174 - accuracy: 0.9288 - val_loss: 0.3044 - val_accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2107 - accuracy: 0.9319 - val_loss: 0.3139 - val_accuracy: 0.8920\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2080 - accuracy: 0.9359 - val_loss: 0.3260 - val_accuracy: 0.8924\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2021 - accuracy: 0.9392 - val_loss: 0.3390 - val_accuracy: 0.8902\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2031 - accuracy: 0.9383 - val_loss: 0.3421 - val_accuracy: 0.8826\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 3ms/step - loss: 0.2011 - accuracy: 0.9381 - val_loss: 0.3641 - val_accuracy: 0.8884\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2873 - accuracy: 0.8878\n",
      "테스트 정확도: 0.888\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
    "]\n",
    "model.fit(\n",
    "    binary_1gram_train_ds.cache(),\n",
    "    epochs=10,\n",
    "    validation_data=binary_1gram_val_ds.cache(),\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3728 - accuracy: 0.8464 - val_loss: 0.2629 - val_accuracy: 0.8954\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2331 - accuracy: 0.9166 - val_loss: 0.2607 - val_accuracy: 0.9002\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2029 - accuracy: 0.9354 - val_loss: 0.2767 - val_accuracy: 0.9010\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1805 - accuracy: 0.9431 - val_loss: 0.2932 - val_accuracy: 0.8990\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1717 - accuracy: 0.9481 - val_loss: 0.3088 - val_accuracy: 0.8974\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1640 - accuracy: 0.9522 - val_loss: 0.3310 - val_accuracy: 0.8964\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1647 - accuracy: 0.9528 - val_loss: 0.3367 - val_accuracy: 0.8966\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.1578 - accuracy: 0.9566 - val_loss: 0.3502 - val_accuracy: 0.8954\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.1577 - accuracy: 0.9583 - val_loss: 0.3558 - val_accuracy: 0.8978\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1460 - accuracy: 0.9603 - val_loss: 0.3747 - val_accuracy: 0.8952\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.2684 - accuracy: 0.9002\n",
      "테스트 정확도: 0.900\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    binary_2gram_train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=binary_2gram_val_ds,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320033 (1.22 MB)\n",
      "Trainable params: 320033 (1.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4881 - accuracy: 0.7934 - val_loss: 0.3055 - val_accuracy: 0.8756\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3183 - accuracy: 0.8686 - val_loss: 0.3216 - val_accuracy: 0.8794\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2884 - accuracy: 0.8816 - val_loss: 0.3091 - val_accuracy: 0.8928\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2658 - accuracy: 0.8891 - val_loss: 0.3368 - val_accuracy: 0.8928\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2460 - accuracy: 0.8989 - val_loss: 0.3480 - val_accuracy: 0.8830\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2356 - accuracy: 0.8993 - val_loss: 0.3480 - val_accuracy: 0.8820\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2355 - accuracy: 0.9011 - val_loss: 0.3650 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2073 - accuracy: 0.9089 - val_loss: 0.3839 - val_accuracy: 0.8742\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2121 - accuracy: 0.9084 - val_loss: 0.3659 - val_accuracy: 0.8816\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2124 - accuracy: 0.9054 - val_loss: 0.3812 - val_accuracy: 0.8820\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.3057 - accuracy: 0.8816\n",
      "테스트 정확도: 0.882\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    tfidf_2gram_train_ds,\n",
    "    epochs=10,\n",
    "    validation_data=tfidf_2gram_val_ds,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling layer 'string_lookup_1' (type StringLookup).\n\nWhen using `output_mode=tf_idf` and `pad_to_max_tokens=False`, you must set the layer's vocabulary before calling it. Either pass a `vocabulary` argument to the layer, or call `adapt` with some sample data.\n\nCall arguments received by layer 'string_lookup_1' (type StringLookup):\n  • inputs=tf.RaggedTensor(values=Tensor(\"text_vectorization_1/StringNGrams/StringNGrams:0\", shape=(None,), dtype=string), row_splits=Tensor(\"text_vectorization_1/StringNGrams/StringNGrams:1\", shape=(None,), dtype=int64))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, ), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtext_vectorization\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(processed_inputs)\n\u001b[1;32m      4\u001b[0m inference_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/layers/preprocessing/index_lookup.py:926\u001b[0m, in \u001b[0;36mIndexLookup._ensure_known_vocab_size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frozen_vocab_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    927\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen using `output_mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    928\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `pad_to_max_tokens=False`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou must set the layer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms vocabulary before calling it. Either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a `vocabulary` argument to the layer, or call `adapt` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith some sample data.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_mode)\n\u001b[1;32m    932\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling layer 'string_lookup_1' (type StringLookup).\n\nWhen using `output_mode=tf_idf` and `pad_to_max_tokens=False`, you must set the layer's vocabulary before calling it. Either pass a `vocabulary` argument to the layer, or call `adapt` with some sample data.\n\nCall arguments received by layer 'string_lookup_1' (type StringLookup):\n  • inputs=tf.RaggedTensor(values=Tensor(\"text_vectorization_1/StringNGrams/StringNGrams:0\", shape=(None,), dtype=string), row_splits=Tensor(\"text_vectorization_1/StringNGrams/StringNGrams:1\", shape=(None,), dtype=int64))"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(1, ), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m raw_text_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor([\n\u001b[1;32m      4\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThat was an excellent movie, I loved it.\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m ])\n\u001b[0;32m----> 6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43minference_model\u001b[49m(raw_text_data)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m긍정적인 리뷰일 확률: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mfloat\u001b[39m(predictions[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inference_model' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"긍정적인 리뷰일 확률: {float(predictions[0] * 100):.2f}%%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot_1 (TFOpLambda)   (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 64)                5128448   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5128513 (19.56 MB)\n",
      "Trainable params: 5128513 (19.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 22:22:56.652434: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-02-15 22:22:57.736422: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa9641b3e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-15 22:22:57.736451: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-02-15 22:22:57.744293: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708003377.808500   56078 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 114s 177ms/step - loss: 0.5594 - accuracy: 0.7156 - val_loss: 0.3928 - val_accuracy: 0.8550\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.3679 - accuracy: 0.8605 - val_loss: 0.3176 - val_accuracy: 0.8742\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 110s 177ms/step - loss: 0.3001 - accuracy: 0.8945 - val_loss: 0.3370 - val_accuracy: 0.8776\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 110s 177ms/step - loss: 0.2568 - accuracy: 0.9126 - val_loss: 0.3106 - val_accuracy: 0.8832\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.2256 - accuracy: 0.9255 - val_loss: 0.3234 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1969 - accuracy: 0.9348 - val_loss: 0.3165 - val_accuracy: 0.8678\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1729 - accuracy: 0.9445 - val_loss: 0.3729 - val_accuracy: 0.8648\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1486 - accuracy: 0.9538 - val_loss: 0.3465 - val_accuracy: 0.8794\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1287 - accuracy: 0.9608 - val_loss: 0.5675 - val_accuracy: 0.8794\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1037 - accuracy: 0.9700 - val_loss: 0.4341 - val_accuracy: 0.8820\n",
      "782/782 [==============================] - 80s 101ms/step - loss: 0.3297 - accuracy: 0.8774\n",
      "테스트 정확도: 0.877\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 43s 65ms/step - loss: 0.5375 - accuracy: 0.7312 - val_loss: 0.4580 - val_accuracy: 0.8298\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.3719 - accuracy: 0.8587 - val_loss: 0.5169 - val_accuracy: 0.8004\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.2938 - accuracy: 0.8938 - val_loss: 0.3227 - val_accuracy: 0.8806\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.2460 - accuracy: 0.9138 - val_loss: 0.7987 - val_accuracy: 0.7744\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.2105 - accuracy: 0.9276 - val_loss: 0.3518 - val_accuracy: 0.8752\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 21s 34ms/step - loss: 0.1853 - accuracy: 0.9373 - val_loss: 0.3802 - val_accuracy: 0.8708\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 21s 33ms/step - loss: 0.1570 - accuracy: 0.9491 - val_loss: 0.4400 - val_accuracy: 0.8722\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.1379 - accuracy: 0.9570 - val_loss: 0.4210 - val_accuracy: 0.8566\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 26s 41ms/step - loss: 0.1169 - accuracy: 0.9633 - val_loss: 0.6196 - val_accuracy: 0.8538\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.1134 - accuracy: 0.9654 - val_loss: 0.5077 - val_accuracy: 0.8664\n",
      "782/782 [==============================] - 12s 13ms/step - loss: 0.3383 - accuracy: 0.8722\n",
      "테스트 정확도: 0.872\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_lstm.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"embeddings_bidir_lstm.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True  True False False False]\n",
      " [ True  True  True  True  True False False]\n",
      " [ True  True False False False False False]], shape=(3, 7), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "some_input = [\n",
    "    [4, 3, 2, 1, 0, 0, 0],\n",
    "    [5, 4, 3, 2, 1, 0, 0],\n",
    "    [2, 1, 0, 0, 0, 0, 0]\n",
    "]\n",
    "mask = embedding_layer.compute_mask(some_input)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 64)                73984     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5194049 (19.81 MB)\n",
      "Trainable params: 5194049 (19.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 23:29:25.605095: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_36/output/_23'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 49s 71ms/step - loss: 0.4402 - accuracy: 0.7913 - val_loss: 0.3423 - val_accuracy: 0.8382\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.2775 - accuracy: 0.8906 - val_loss: 0.2907 - val_accuracy: 0.8818\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.2177 - accuracy: 0.9179 - val_loss: 0.3116 - val_accuracy: 0.8708\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.1656 - accuracy: 0.9397 - val_loss: 0.3124 - val_accuracy: 0.8810\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.1277 - accuracy: 0.9548 - val_loss: 0.3718 - val_accuracy: 0.8804\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 24s 38ms/step - loss: 0.0977 - accuracy: 0.9665 - val_loss: 0.3955 - val_accuracy: 0.8764\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.0755 - accuracy: 0.9757 - val_loss: 0.4743 - val_accuracy: 0.8698\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 22s 34ms/step - loss: 0.0609 - accuracy: 0.9797 - val_loss: 0.4612 - val_accuracy: 0.8770\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 23s 36ms/step - loss: 0.0444 - accuracy: 0.9863 - val_loss: 0.5034 - val_accuracy: 0.8678\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 23s 37ms/step - loss: 0.0420 - accuracy: 0.9867 - val_loss: 0.6515 - val_accuracy: 0.8546\n",
      "782/782 [==============================] - 17s 19ms/step - loss: 0.3024 - accuracy: 0.8735\n",
      "테스트 정확도: 0.874\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=256,\n",
    "    mask_zero=True,\n",
    ")(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_lstm_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"embeddings_bidir_lstm_with_masking.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-15 23:40:55--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "nlp.stanford.edu (nlp.stanford.edu) 해석 중... 171.64.67.140\n",
      "다음으로 연결 중: nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 302 Found\n",
      "위치: https://nlp.stanford.edu/data/glove.6B.zip [따라감]\n",
      "--2024-02-15 23:40:55--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "다음으로 연결 중: nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 301 Moved Permanently\n",
      "위치: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [따라감]\n",
      "--2024-02-15 23:40:56--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "downloads.cs.stanford.edu (downloads.cs.stanford.edu) 해석 중... 171.64.64.22\n",
      "다음으로 연결 중: downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 862182613 (822M) [application/zip]\n",
      "저장 위치: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  5.10MB/s    / 2m 46s   \n",
      "\n",
      "2024-02-15 23:43:43 (4.95 MB/s) - ‘glove.6B.zip’ 저장함 [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip -q glove.6B.zip\n",
    "!rm -f glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 벡터 개수: 400000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"단어 벡터 개수: {len(embeddings_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirecti  (None, 64)                34048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2034113 (7.76 MB)\n",
      "Trainable params: 34113 (133.25 KB)\n",
      "Non-trainable params: 2000000 (7.63 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 47s 65ms/step - loss: 0.5733 - accuracy: 0.6966 - val_loss: 0.4716 - val_accuracy: 0.7818\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 38s 60ms/step - loss: 0.4517 - accuracy: 0.7956 - val_loss: 0.3891 - val_accuracy: 0.8324\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 20s 31ms/step - loss: 0.4028 - accuracy: 0.8212 - val_loss: 0.3939 - val_accuracy: 0.8310\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.3703 - accuracy: 0.8401 - val_loss: 0.3781 - val_accuracy: 0.8388\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 37s 59ms/step - loss: 0.3457 - accuracy: 0.8507 - val_loss: 0.3650 - val_accuracy: 0.8540\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.3243 - accuracy: 0.8642 - val_loss: 0.3936 - val_accuracy: 0.8386\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 38s 61ms/step - loss: 0.3052 - accuracy: 0.8729 - val_loss: 0.3362 - val_accuracy: 0.8598\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 36s 58ms/step - loss: 0.2877 - accuracy: 0.8813 - val_loss: 0.2993 - val_accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 19s 31ms/step - loss: 0.2711 - accuracy: 0.8891 - val_loss: 0.3097 - val_accuracy: 0.8760\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 22s 35ms/step - loss: 0.2593 - accuracy: 0.8948 - val_loss: 0.3321 - val_accuracy: 0.8780\n",
      "782/782 [==============================] - 12s 11ms/step - loss: 0.3021 - accuracy: 0.8744\n",
      "테스트 정확도: 0.874\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 준비\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim      # 입력 토큰 벡터의 크기\n",
    "        self.dense_dim = dense_dim      # 내부 밀집 층의 크기\n",
    "        self.num_heads = num_heads      # 어텐션 헤드의 개수\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim = embed_dim,\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-7)\n",
    "        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-7)\n",
    "        \n",
    "    # call 메소드에서 연산을 수행한다.\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Embedding 층에서 생성하는 마스크는 2D이지만 어텐션 층은 3D 또는 4D를 기대하므로 랭크를 높인다.\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask,\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    # 모델을 저장할 수 있도록 직렬화를 구현한다.\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5664033 (21.61 MB)\n",
      "Trainable params: 5664033 (21.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 19:41:48.736001: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7404943ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-16 19:41:48.736032: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-02-16 19:41:48.743705: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-16 19:41:48.760317: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708080108.809391    9184 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 40s 58ms/step - loss: 0.5055 - accuracy: 0.7595 - val_loss: 0.3515 - val_accuracy: 0.8458\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 31s 49ms/step - loss: 0.3500 - accuracy: 0.8512 - val_loss: 0.3173 - val_accuracy: 0.8658\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 29s 47ms/step - loss: 0.3111 - accuracy: 0.8666 - val_loss: 0.3108 - val_accuracy: 0.8738\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.2833 - accuracy: 0.8828 - val_loss: 0.3028 - val_accuracy: 0.8738\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 28s 45ms/step - loss: 0.2550 - accuracy: 0.8929 - val_loss: 0.2984 - val_accuracy: 0.8808\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.2284 - accuracy: 0.9087 - val_loss: 0.3162 - val_accuracy: 0.8704\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.1942 - accuracy: 0.9227 - val_loss: 0.2979 - val_accuracy: 0.8834\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 28s 44ms/step - loss: 0.1718 - accuracy: 0.9347 - val_loss: 0.3216 - val_accuracy: 0.8770\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.1463 - accuracy: 0.9445 - val_loss: 0.3192 - val_accuracy: 0.8808\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.1210 - accuracy: 0.9546 - val_loss: 0.3716 - val_accuracy: 0.8644\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0932 - accuracy: 0.9664 - val_loss: 0.3718 - val_accuracy: 0.8702\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.0781 - accuracy: 0.9718 - val_loss: 0.3707 - val_accuracy: 0.8792\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0618 - accuracy: 0.9785 - val_loss: 0.6058 - val_accuracy: 0.8368\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 27s 44ms/step - loss: 0.0502 - accuracy: 0.9826 - val_loss: 0.4209 - val_accuracy: 0.8780\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0400 - accuracy: 0.9861 - val_loss: 0.5045 - val_accuracy: 0.8646\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0326 - accuracy: 0.9886 - val_loss: 0.5221 - val_accuracy: 0.8740\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0284 - accuracy: 0.9900 - val_loss: 0.5763 - val_accuracy: 0.8626\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0226 - accuracy: 0.9919 - val_loss: 0.5994 - val_accuracy: 0.8580\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0220 - accuracy: 0.9919 - val_loss: 0.6736 - val_accuracy: 0.8560\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 27s 43ms/step - loss: 0.0186 - accuracy: 0.9935 - val_loss: 0.6172 - val_accuracy: 0.8706\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.3063 - accuracy: 0.8758\n",
      "테스트 정확도: 0.876\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder}   # 모델 사용 시 사용자 정의 TransformerEncoder 클래스를 지정한다.\n",
    ")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    # 위치 임베딩의 단점은 시퀀스 길이를 미리 알아야 한다는 것이다.\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 토큰 인덱스를 위한 Embedding 층을 준비한다.\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim,\n",
    "        )\n",
    "        # 토큰 위치를 위한 Embedding 층을 준비한다.\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        # 두 임베딩 벡터를 더한다.\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    # 모델 저장을 위한 직렬화를 구현한다.\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_1 (Po  (None, None, 256)         5273600   \n",
      " sitionalEmbedding)                                              \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817633 (22.19 MB)\n",
      "Trainable params: 5817633 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 16:35:27.655032: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f67981f6900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-17 16:35:27.655077: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-02-17 16:35:27.662883: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-02-17 16:35:27.681217: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708155327.733506   52877 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 43s 63ms/step - loss: 0.5159 - accuracy: 0.7467 - val_loss: 0.3898 - val_accuracy: 0.8158\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 34s 54ms/step - loss: 0.2954 - accuracy: 0.8741 - val_loss: 0.2788 - val_accuracy: 0.8876\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.2336 - accuracy: 0.9046 - val_loss: 0.3839 - val_accuracy: 0.8450\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1906 - accuracy: 0.9259 - val_loss: 0.2853 - val_accuracy: 0.8916\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.1622 - accuracy: 0.9376 - val_loss: 0.4403 - val_accuracy: 0.8714\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 32s 50ms/step - loss: 0.1374 - accuracy: 0.9477 - val_loss: 0.3832 - val_accuracy: 0.8588\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 31s 50ms/step - loss: 0.1150 - accuracy: 0.9566 - val_loss: 0.4561 - val_accuracy: 0.8776\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0917 - accuracy: 0.9657 - val_loss: 0.4035 - val_accuracy: 0.8778\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0738 - accuracy: 0.9742 - val_loss: 0.3997 - val_accuracy: 0.8722\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0551 - accuracy: 0.9799 - val_loss: 0.5129 - val_accuracy: 0.8700\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.0466 - accuracy: 0.9852 - val_loss: 0.5779 - val_accuracy: 0.8738\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0358 - accuracy: 0.9880 - val_loss: 0.7725 - val_accuracy: 0.8624\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 30s 49ms/step - loss: 0.0264 - accuracy: 0.9909 - val_loss: 0.7750 - val_accuracy: 0.8716\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0223 - accuracy: 0.9930 - val_loss: 0.7492 - val_accuracy: 0.8710\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0214 - accuracy: 0.9926 - val_loss: 1.0512 - val_accuracy: 0.8666\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 1.0023 - val_accuracy: 0.8650\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0146 - accuracy: 0.9952 - val_loss: 1.0656 - val_accuracy: 0.8684\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0132 - accuracy: 0.9961 - val_loss: 1.3856 - val_accuracy: 0.8734\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 1.3358 - val_accuracy: 0.8674\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 30s 48ms/step - loss: 0.0107 - accuracy: 0.9973 - val_loss: 1.2071 - val_accuracy: 0.8660\n",
      "782/782 [==============================] - 15s 19ms/step - loss: 0.2890 - accuracy: 0.8779\n",
      "테스트 정확도: 0.878\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None, ), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    int_train_ds,\n",
    "    validation_data=int_val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding}\n",
    ")\n",
    "print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-17 17:43:01--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "storage.googleapis.com (storage.googleapis.com) 해석 중... 34.64.4.91, 34.64.4.123, 34.64.4.27, ...\n",
      "다음으로 연결 중: storage.googleapis.com (storage.googleapis.com)|34.64.4.91|:80... 연결했습니다.\n",
      "HTTP 요청을 보냈습니다. 응답 기다리는 중... 200 OK\n",
      "길이: 2638744 (2.5M) [application/zip]\n",
      "저장 위치: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   2.52M  --.-KB/s    / 0.07s    \n",
      "\n",
      "2024-02-17 17:43:02 (37.0 MB/s) - ‘spa-eng.zip’ 저장함 [2638744/2638744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip\n",
    "!rm -f spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The smell was offensive.', '[start] El olor era desagradable. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 00:32:11.790466: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.790675: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.790778: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791042: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791228: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-18 00:32:11.791678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:03:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
    "    )\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "# 영어 층\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "# 스페인어 층\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,     # 훈련하는 동안 한 스텝 앞선 문장이 필요하기 때문에 토큰 하나가 추가된 스페인어 문장을 생성한다.\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "# 각 언어의 어휘 사전을 만든다.\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],     # 입력 스페인어 문장은 마지막 토큰을 포함하지 않기 때문에 입력과 타깃 길이가 같다.\n",
    "    }, spa[:, 1:])                  # 타깃 스페인어 문장은 한 스텝 앞의 문장이다. 길이는 입력과 같다.\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=8)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"english\"].shape: (64, 20)\n",
      "inputs[\"spanish\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 00:32:47.163082: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs[\\\"english\\\"].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs[\\\"spanish\\\"].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length, ), dtype=\"int64\")\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None, ), dtype=\"int64\", name=\"english\")     # 영어 소스 문장이 여기에 입력된다. 입력 이름을 지정함으로써 입력 딕셔너리로 모델을 훈련할 수 있다.\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)     # 마스킹은 이 방식에서 중요하다.\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode=\"sum\"\n",
    ")(x)    # 인코딩된 소스 문장은 양방향 GRU의 마지막 출력이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None, ), dtype=\"int64\", name=\"spanish\")    # 스페인어 타깃 시퀀스가 여기에 입력된다.\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)    # 마스킹은 필수이다.\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)                            # 인코딩된 소스 시퀀스는 디코더 GRU의 초기 상태가 된다.\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)        # 다음 토큰을 예측한다.\n",
    "# 엔드-투-엔드 모델은 소스 시퀀스와 타깃 시퀀스를 한 스텝 앞의 타깃 시퀀스에 매핑한다.\n",
    "seq2seq_rnn = keras.Model(inputs=[source, past_target], outputs=target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 00:32:57.785105: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
      "type_id: TFT_OPTIONAL\n",
      "args {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_TENSOR\n",
      "    args {\n",
      "      type_id: TFT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\tfor Tuple type infernce function 0\n",
      "\twhile inferring type of node 'cond_35/output/_22'\n",
      "2024-02-18 00:32:58.496365: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2024-02-18 00:32:59.527668: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7682865a10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-18 00:32:59.527696: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-02-18 00:32:59.534392: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708183979.598704    6934 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 88s 61ms/step - loss: 4.6846 - accuracy: 0.3193 - val_loss: 3.9046 - val_accuracy: 0.3865\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 72s 55ms/step - loss: 3.7190 - accuracy: 0.4167 - val_loss: 3.2981 - val_accuracy: 0.4628\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 75s 58ms/step - loss: 3.2073 - accuracy: 0.4739 - val_loss: 2.8680 - val_accuracy: 0.5182\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 73s 56ms/step - loss: 2.8477 - accuracy: 0.5148 - val_loss: 2.6100 - val_accuracy: 0.5536\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 72s 55ms/step - loss: 2.5733 - accuracy: 0.5477 - val_loss: 2.4218 - val_accuracy: 0.5813\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 73s 56ms/step - loss: 2.3515 - accuracy: 0.5756 - val_loss: 2.3014 - val_accuracy: 0.5988\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 72s 55ms/step - loss: 2.1632 - accuracy: 0.6002 - val_loss: 2.2022 - val_accuracy: 0.6155\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 72s 55ms/step - loss: 2.0061 - accuracy: 0.6210 - val_loss: 2.1312 - val_accuracy: 0.6254\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 72s 56ms/step - loss: 1.8693 - accuracy: 0.6397 - val_loss: 2.0843 - val_accuracy: 0.6330\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 75s 57ms/step - loss: 1.7498 - accuracy: 0.6565 - val_loss: 2.0352 - val_accuracy: 0.6407\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 73s 56ms/step - loss: 1.6442 - accuracy: 0.6710 - val_loss: 1.9979 - val_accuracy: 0.6473\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 72s 56ms/step - loss: 1.5516 - accuracy: 0.6846 - val_loss: 1.9783 - val_accuracy: 0.6516\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 74s 57ms/step - loss: 1.4694 - accuracy: 0.6961 - val_loss: 1.9510 - val_accuracy: 0.6564\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 74s 57ms/step - loss: 1.3973 - accuracy: 0.7070 - val_loss: 1.9382 - val_accuracy: 0.6579\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 74s 57ms/step - loss: 1.3355 - accuracy: 0.7165 - val_loss: 1.9231 - val_accuracy: 0.6609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f76a81b3d30>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "seq2seq_rnn.fit(\n",
    "    train_ds,\n",
    "    epochs=15,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "This is nonsense.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] esto es [UNK] [end]\n",
      "-\n",
      "You'll learn how to do it sooner or later.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] aprenderás a hacerlo tarde o temprano [end]\n",
      "-\n",
      "They haven't come back home yet.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] todavía no han venido de casa [end]\n",
      "-\n",
      "Sorry for the harsh words.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] [UNK] por el proyecto de palabras [end]\n",
      "-\n",
      "I'm leaving on Sunday.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] me voy a la playa [end]\n",
      "-\n",
      "I thought it'd be more comfortable if we sat here.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] pensé que sería más tiempo para que nos [UNK] aquí [end]\n",
      "-\n",
      "Tom is proud of his daughter.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] tom está orgulloso de su hija [end]\n",
      "-\n",
      "We are out of danger.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] estamos de acuerdo [end]\n",
      "-\n",
      "Her house is close to the park.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] su casa está cerca del parque [end]\n",
      "-\n",
      "I don't want to talk to you.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] no quiero hablar contigo [end]\n",
      "-\n",
      "It's healthy and normal.\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] es hora y [UNK] [end]\n",
      "-\n",
      "I hate rules.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] odio las reglas [end]\n",
      "-\n",
      "She traveled around the world.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] ella viajó por todo el mundo [end]\n",
      "-\n",
      "He's probably wrong.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] Él probablemente está mal [end]\n",
      "-\n",
      "She is not a good person.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] ella no es una buena persona [end]\n",
      "-\n",
      "I'm all tuckered out.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] estoy todo el [UNK] [end]\n",
      "-\n",
      "Do I remind you of the one of the guys you left behind?\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] te [UNK] de la cabeza cuando se le [UNK] [end]\n",
      "-\n",
      "Tom wore a white jacket.\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] tom llevaba un vestido de rojo [end]\n",
      "-\n",
      "Tom doesn't know what Mary meant.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "[start] tom no sabe lo que maría hizo mary [end]\n",
      "-\n",
      "Do come in.\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "[start] pasa [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예측된 인덱스를 문자열 토큰으로 변환하기 위한 딕셔너리를 준비한다.\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"        # 시드 토큰\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        # 다음 토큰을 샘플링한다.\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        # 다음 토큰 예측을 문자열로 바꾸고 생성된 문장에 추가한다.\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += (\" \" + sampled_token)\n",
    "        if sampled_token == \"[end]\":    # 종료 조건: 최대 길이에 도달하거나 종료 문자가 생성된 경우\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
