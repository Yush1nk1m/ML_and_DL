# 케라스 창시자에게 배우는 딥러닝
## 12장, *생성 모델을 위한 딥러닝*

예술 창작의 대부분은 간단한 패턴 인식과 기교로 만들어진다. 여기에 AI가 필요하다. 사람의 지각, 언어, 예술 작품은 모두 통계적 구조를 가지며 딥러닝 알고리즘은 이 구조를 학습하는 데 뛰어나다. 머신 러닝 모델은 이미지, 음악, 글의 통계적 **잠재 공간**(latent space)을 학습할 수 있다. 그 다음 이 공간에서 샘플을 뽑아 새로운 예술 작품을 만들 수 있다. 이런 샘플링 자체는 수학 연산에 불과하지만 숙련된 예술가가 사용하면 알고리즘 창작 자체가 의미 있고 아름다운 것으로 바뀔 수 있다.



## 12.1 텍스트 생성

이 절에서는 순환 신경망으로 시퀀스 데이터를 생성하는 방법을 알아볼 것이다. 텍스트 생성을 예로 들지만 동일한 기법으로 어떤 종류의 시퀀스 데이터도 생성할 수 있다. 음표에 적용하여 새로운 음악을 만들거나 연속된 붓질 시퀀스에 적용하여 한 획씩 그림을 그릴 수도 있다.

### 12.1.1 시퀀스 생성을 위한 딥러닝 모델의 간단한 역사

순환 네트워크를 사용하여 데이터를 성공적으로 생성한 애플리케이션은 2016년이 되어서야 주류가 되기 시작했다.

2002년 더글라스 에크는 스위스의 슈미드후버의 연구실에서 LSTM을 음악 생성이 최초로 적용하여 가능성 있는 결과를 얻어냈고, 2016년에는 구글에서 마젠타라는 새로운 연구 그룹을 만들어 최신 딥러닝 기술로 멋진 음악을 만드는 것에 집중하였다.

2000년대 후반과 2010년대 초반에 알렉스 그레이브스는 순환 네트워크를 사용하여 시퀀스 데이터를 생성하는 데 아주 중요한 선구적인 일을 하였다. 2013년에 펜 위치를 기록한 시계열 데이터로 순환 네트워크와 완전 연결 네트워크를 혼합한 네트워크로 사람이 쓴 것 같은 손글씨를 생성했고, 이 작업이 전환점이 되었다.

2015년에서 2017년 사이에 순환 신경망은 텍스트와 대화 생성, 음악 생성, 음성 합성에 성공적으로 사용되었다.

그 다음 2017년~2018년 즈음에 트랜스포머 아키텍처가 자연어 처리 지도 학습 작업뿐만 아니라 시퀀스 생성 모델, 특히 언어 모델링(단어 수준의 텍스트 생성)에서 순환 신경망을 압도하기 시작했다. 가장 잘 알려진 생성 트랜스포머의 예는 1,750억 개의 파라미터를 가진 텍스트 생성 모델인 GPT-3이다.

### 12.1.2 시퀀스 데이터를 어떻게 생성할까?

딥러닝에서 시퀀스 데이터를 생성하는 일반적인 방법은 이전 토큰을 입력으로 사용하여 시퀀스의 다음 1개 또는 몇 개의 토큰을 트랜스포머나 RNN으로 예측하는 것이다. 텍스트 데이터를 다룰 때 다음 토큰은 보통 단어 또는 글자이다. 이전 토큰들이 주어졌을 때 다음 토큰의 확률을 모델링할 수 있는 네트워크를 **언어 모델**(language model)이라고 부른다. 언어 모델은 언어의 통계적 구조인 잠재 공간을 탐색한다.

언어 모델 훈련 후에는 이 모델에서 샘플링을 할 수 있다(새로운 시퀀스를 생성한다). 초기 텍스트 문자열인 **조건 데이터**(conditioning data)를 주입하고 새로운 글자나 단어를 생성한다(한 번에 여러 개의 토큰을 생성할 수도 있다). 생성된 출력은 다시 입력 데이터로 추가된다. 이 과정을 여러 번 반복한다. 이런 반복을 통해 모델이 훈련한 데이터 구조가 반영된 임의의 길이를 가진 시퀀스를 생성할 수 있다. 이런 시퀀스는 사람이 쓴 문장과 거의 비슷하다.

### 12.1.3 샘플링 전략의 중요성

텍스트 생성 시 다음 문자를 선택하는 방법은 아주 중요하다. 단순한 방법은 항상 가장 높은 확률을 가진 글자를 선택하는 **탐욕적 샘플링**(greedy sampling)이다. 이 방법은 반복적이고 예상 가능한 문자열을 만들기 때문에 논리적인 언어처럼 보이지 않는다. 이 외에 다음 단어를 확률 분포에서 샘플링하는 과정에 무작위성을 주입하는 **확률적 샘플링**(stochastic sampling)이 있다. 만약 어떤 단어가 문장의 다음 단어가 될 확률이 0.3이라면 확률적 샘플링 모델의 30% 정도는 이 단어를 선택한다. 탐욕적 샘플링을 확률적 샘플링의 한 종류로 설명할 수도 있는데, 한 단어만 확률이 1이고 나머지는 모두 0인 확률 분포를 가지는 경우이다.

모델의 소프트맥스 출력은 확률적 샘플링에 사용하기 좋다. 훈련 데이터에는 없으나 실제 같은 새로운 문장을 만들기 때문에 더 흥미롭게 보이는 문장이 만들어지고 이따금씩 창의성을 보이기도 한다. 그러나 샘플링 과정에서 무작위성을 조절할 방법이 없다는 문제점이 존재한다.

만약 무작위성이 최대인(즉, 엔트로피가 최대인) 균등 확률 분포에서 다음 단어를 추출한다고 가정하자. 이 경우엔 흥미로운 것들을 생성할 수 없기 때문에 문제가 있다. 반대로 무작위성이 최소라면 탐욕적 샘플링이 되어 최소의 엔트로피를 가진다. 모델의 소프트맥스 출력인 실제 확률 분포에서 샘플링하는 것은 이 두 극단적인 케이스의 중간에 위치해 있다. 작은 엔트로피는 예상 가능한 구조를 가진 시퀀스를 생성하여 더 실제처럼 보일 수 있고, 높은 엔트로피는 놀랍고 창의적인 시퀀스를 만든다. 생성 모델에서 샘플링을 할 때 생성 과정에 무작위성의 양을 바꾸어 시도해 보는 것이 좋다. 흥미라는 것은 매우 주관적이므로 최적의 엔트로피 값을 미리 알 수 없기 때문에 사람이 판단해야 한다.

샘플링 과정에서 확률의 양을 조절하기 위해 **소프트맥스 온도**(softmax temperature)라는 파라미터를 사용한다. 이 파라미터는 샘플링에 사용되는 확률 분포의 엔트로피를 나타낸다. 얼마나 놀라운, 또는 예상되는 단어를 선택할지 결정한다. `temperature` 값이 주어지면 다음과 같이 가중치를 적용하여 모델의 소프트맥스 출력인 원본 확률 분포에서 새로운 확률 분포를 계산한다.

**코드 12-1. 다른 온도 값을 사용하여 확률 분포의 가중치 바꾸기**
```
import numpy as np

# original_distribution은 전체 합이 1인 1D 넘파이 배열이다.
# temperature는 출력 분포의 엔트로피의 양을 결정한다.
def reweight_distribution(original_distribution, temperature=0.5):
    distribution = np.log(original_distribution) / temperature
    distribution = np.exp(distribution)
    return distribution / np.sum(distribution)  # 원본 분포의 가중치를 변경하며 반환한다. 이 분포의 합은 1이 아닐 수 있으므로 새로운 분포의 합으로 나눈다.
```

높은 온도는 엔트로피가 높은 샘플링 분포를 만들어 더 놀랍고 생소한 데이터를 생성한다. 반면 낮은 온도는 무작위성이 낮기 때문에 예상할 수 있는 데이터를 생성한다.